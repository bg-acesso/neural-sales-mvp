2025-12-30 00:58:40 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 00:59:23 | INFO     | ‚úÖ Input capturado: 678 caracteres
2025-12-30 00:59:23 | INFO     | Iniciando processamento de 678 caracteres
2025-12-30 00:59:23 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 00:59:23 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 00:59:23 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 00:59:27 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 00:59:27 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:05:01 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:06:24 | INFO     | ‚úÖ Input capturado: 604 caracteres
2025-12-30 01:06:24 | INFO     | Iniciando processamento de 604 caracteres
2025-12-30 01:06:24 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:06:24 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:06:24 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:06:28 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:06:28 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:06:29 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:06:29 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:06:30 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:06:30 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:06:31 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:06:31 | INFO     | üé® Gerando imagem: Isometric 3D render of connections overcoming fear...
2025-12-30 01:06:34 | INFO     | ‚úÖ Imagem salva: galeria_refinaria\post_2025_12_30__01_06_34.png
2025-12-30 01:06:34 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:06:34 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:06:38 | INFO     | üìÅ Criando pasta: galeria_refinaria\post_2025_12_30__01_06_38
2025-12-30 01:06:38 | INFO     | ‚úÖ Post salvo: galeria_refinaria\post_2025_12_30__01_06_38\post.txt
2025-12-30 01:06:38 | INFO     | ‚úÖ Processamento conclu√≠do com sucesso
2025-12-30 01:11:28 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:11:41 | INFO     | ‚úÖ Input capturado: 604 caracteres
2025-12-30 01:11:41 | INFO     | Iniciando processamento de 604 caracteres
2025-12-30 01:11:41 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:11:41 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:12:36 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:12:40 | INFO     | ‚úÖ Input capturado: 604 caracteres
2025-12-30 01:12:40 | INFO     | Iniciando processamento de 604 caracteres
2025-12-30 01:12:40 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:12:40 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:12:40 | INFO     | 
LiteLLM completion() model= llama-4-scout-17b-16e-instruct; provider = groq
2025-12-30 01:14:02 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:14:29 | WARNING  | Texto muito curto: 28 caracteres
2025-12-30 01:14:31 | INFO     | ‚úÖ Input capturado: 28 caracteres
2025-12-30 01:14:31 | INFO     | Iniciando processamento de 28 caracteres
2025-12-30 01:14:34 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:14:34 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:14:35 | ERROR    | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:14:35 | ERROR    | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:14:36 | ERROR    | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:14:36 | ERROR    | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:14:36 | ERROR    | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:14:36 | ERROR    | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:14:36 | ERROR    | Erro fatal: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 969, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 422, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 303, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 209, in call
    return self._handle_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 510, in _handle_completion
    raise e from e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 422, in _handle_completion
    response: ChatCompletion = self.client.chat.completions.create(**params)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-12-30 01:15:14 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:15:19 | WARNING  | Texto muito curto: 28 caracteres
2025-12-30 01:15:20 | INFO     | ‚úÖ Input capturado: 28 caracteres
2025-12-30 01:15:20 | INFO     | Iniciando processamento de 28 caracteres
2025-12-30 01:15:20 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:15:20 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:17:41 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:17:45 | WARNING  | Texto muito curto: 41 caracteres
2025-12-30 01:17:46 | INFO     | ‚úÖ Input capturado: 41 caracteres
2025-12-30 01:17:46 | INFO     | Iniciando processamento de 41 caracteres
2025-12-30 01:17:46 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:17:46 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:19:14 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:19:28 | INFO     | ‚úÖ Input capturado: 51 caracteres
2025-12-30 01:19:28 | INFO     | Iniciando processamento de 51 caracteres
2025-12-30 01:19:28 | INFO     | ‚úÖ 4 agentes criados (modelo: groq/llama-3.1-8b-instant)
2025-12-30 01:19:28 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:19:28 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:30 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:30 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:31 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:31 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:32 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:32 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:33 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:33 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:33 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:33 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:33 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:34 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:34 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:19:34 | INFO     | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
2025-12-30 01:19:34 | ERROR    | Erro fatal: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5885, Requested 443. Please try again in 3.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 303, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 282, in get_llm_response
    raise ValueError("Invalid response from LLM call - None or empty.")
ValueError: Invalid response from LLM call - None or empty.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 303, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 282, in get_llm_response
    raise ValueError("Invalid response from LLM call - None or empty.")
ValueError: Invalid response from LLM call - None or empty.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 217, in _make_common_sync_call
    response = sync_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 979, in post
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 961, in post
    response.raise_for_status()
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 2126, in completion
    response = base_llm_http_handler.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 520, in completion
    response = self._make_common_sync_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 242, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5885, Requested 443. Please try again in 3.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 1033, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 411, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 291, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1669, in call
    result = self._handle_non_streaming_response(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1150, in _handle_non_streaming_response
    response = litellm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1405, in wrapper
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1274, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 4080, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5885, Requested 443. Please try again in 3.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-12-30 01:20:11 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:20:19 | WARNING  | Texto muito curto: 41 caracteres
2025-12-30 01:20:20 | INFO     | ‚úÖ Input capturado: 41 caracteres
2025-12-30 01:20:20 | INFO     | Iniciando processamento de 41 caracteres
2025-12-30 01:20:20 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:20:20 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:23:16 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 01:23:22 | WARNING  | Texto muito curto: 41 caracteres
2025-12-30 01:23:23 | INFO     | ‚úÖ Input capturado: 41 caracteres
2025-12-30 01:23:23 | INFO     | Iniciando processamento de 41 caracteres
2025-12-30 01:23:23 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 01:23:23 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 01:23:23 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:23:27 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:23:27 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:23:28 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:23:28 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:23:28 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:23:28 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:23:29 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:23:29 | INFO     | üé® Gerando imagem: Abstract isometric 3d render of neural connections with grad...
2025-12-30 01:23:32 | INFO     | ‚úÖ Imagem salva: galeria_refinaria\post_2025_12_30__01_23_32.png
2025-12-30 01:23:32 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 01:23:33 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 01:24:38 | INFO     | üìÅ Criando pasta: galeria_refinaria\post_2025_12_30__01_24_38
2025-12-30 01:24:38 | INFO     | ‚úÖ Post salvo: galeria_refinaria\post_2025_12_30__01_24_38\post.txt
2025-12-30 01:24:38 | INFO     | ‚úÖ Input original salvo: galeria_refinaria\post_2025_12_30__01_24_38\input_original.txt
2025-12-30 01:24:38 | INFO     | ‚úÖ Estrutura salva: galeria_refinaria\post_2025_12_30__01_24_38\estrutura.txt
2025-12-30 01:24:38 | INFO     | ‚úÖ Prompt de imagem salvo: galeria_refinaria\post_2025_12_30__01_24_38\prompt_imagem.txt
2025-12-30 01:24:38 | INFO     | ‚úÖ Imagem movida para: galeria_refinaria\post_2025_12_30__01_24_38\imagem.png
2025-12-30 01:24:38 | INFO     | ‚úÖ Metadata salva: galeria_refinaria\post_2025_12_30__01_24_38\metadata.json
2025-12-30 01:24:38 | INFO     | ‚úÖ README criado: galeria_refinaria\post_2025_12_30__01_24_38\README.md
2025-12-30 01:24:38 | INFO     | ‚úÖ Processamento conclu√≠do com sucesso
01:37:59 | INFO     | __main__        | ‚úÖ Input capturado: 29 caracteres
01:37:59 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
01:37:59 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
01:37:59 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:03 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:03 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:06 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:06 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:08 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:08 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:09 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:09 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:13 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:13 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:15 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:15 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:16 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:17 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:17 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:17 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:18 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:18 | INFO     | __main__        | üé® Gerando imagem: isometric 3d render abstract tech shapes gradient blue green...
01:38:21 | INFO     | __main__        | ‚úÖ Imagem salva: galeria_refinaria\img_20251230_013821.png
01:38:21 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:38:21 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:38:21 | INFO     | __main__        | ‚úÖ Resultado salvo em: galeria_refinaria\post_20251230_013821
01:38:21 | INFO     | __main__        | ‚úÖ Pipeline conclu√≠do com sucesso
01:44:46 | INFO     | __main__        | ‚úÖ Input capturado: 565 caracteres
01:44:46 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
01:44:46 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
01:44:47 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:44:50 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:44:50 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:44:53 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:44:53 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:44:54 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:44:54 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:44:57 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:44:57 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:44:59 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:44:59 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:45:01 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:45:01 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:45:56 | INFO     | __main__        | ‚úÖ Input capturado: 565 caracteres
01:45:56 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
01:45:56 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
01:45:56 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:00 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:00 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:03 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:03 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:04 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:04 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:07 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:07 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:09 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:09 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:11 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:11 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:12 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:12 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:13 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:13 | INFO     | __main__        | üé® Gerando imagem: isometric 3d render geometric shapes, blue green gradient, s...
01:46:17 | INFO     | __main__        | ‚úÖ Imagem salva: galeria_refinaria\img_20251230_014617.png
01:46:17 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
01:46:17 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
01:46:17 | INFO     | __main__        | ‚úÖ Resultado salvo em: galeria_refinaria\post_20251230_014617
01:46:17 | INFO     | __main__        | ‚úÖ Pipeline conclu√≠do com sucesso
03:06:49 | INFO     | __main__        | ‚úÖ Input capturado: 55 caracteres
03:06:49 | ERROR    | __main__        | Erro fatal: 1 validation error for Agent
goal
  Input should be a valid string [type=string_type, input_value={'output_format': 'JSON e...ts para credibilidade']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 1002, in main
    equipe = criar_equipe_elite()
             ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 254, in criar_equipe_elite
    estrategista = Agent(
                   ^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for Agent
goal
  Input should be a valid string [type=string_type, input_value={'output_format': 'JSON e...ts para credibilidade']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
03:14:25 | INFO     | __main__        | ‚úÖ Input capturado: 46 caracteres
03:14:25 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:14:25 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:14:25 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:14:29 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:14:29 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:14:33 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:14:33 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:14:34 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:14:34 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:14:37 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:14:37 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:14:43 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:14:43 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:28 | INFO     | __main__        | ‚úÖ Input capturado: 46 caracteres
03:17:28 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:17:28 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:17:28 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:32 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:17:32 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:35 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:17:35 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:37 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:17:37 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:39 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:17:40 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:42 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:17:42 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:17:42 | ERROR    | __main__        | Erro fatal: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10347, Requested 1947. Please try again in 1.47s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 217, in _make_common_sync_call
    response = sync_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 979, in post
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 961, in post
    response.raise_for_status()
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 2126, in completion
    response = base_llm_http_handler.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 520, in completion
    response = self._make_common_sync_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 242, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10347, Requested 1947. Please try again in 1.47s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 411, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 291, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1669, in call
    result = self._handle_non_streaming_response(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1150, in _handle_non_streaming_response
    response = litellm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1405, in wrapper
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1274, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 4080, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10347, Requested 1947. Please try again in 1.47s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

03:18:29 | INFO     | __main__        | ‚úÖ Input capturado: 46 caracteres
03:18:29 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:18:29 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:18:29 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:18:33 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:18:33 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:18:35 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:18:35 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:18:36 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:18:36 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:18:39 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:18:39 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:18:42 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:18:42 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:27:34 | INFO     | __main__        | ‚úÖ Input capturado: 61 caracteres
03:27:34 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:27:34 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:27:34 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-8b-versatile; provider = groq
03:29:47 | INFO     | __main__        | ‚úÖ Input capturado: 37 caracteres
03:29:47 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:29:47 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:29:47 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama3-8b-8192; provider = groq
03:31:41 | INFO     | __main__        | ‚úÖ Input capturado: 18 caracteres
03:31:44 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:31:44 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:31:45 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:31:45 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:31:45 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:31:45 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:31:45 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:31:45 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:31:45 | ERROR    | __main__        | Erro fatal: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 422, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 303, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 209, in call
    return self._handle_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 510, in _handle_completion
    raise e from e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 422, in _handle_completion
    response: ChatCompletion = self.client.chat.completions.create(**params)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:32:56 | INFO     | __main__        | ‚úÖ Input capturado: 23 caracteres
03:32:56 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:32:56 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:32:56 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama3-8b-8192; provider = groq
03:32:57 | ERROR    | __main__        | Erro fatal: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}
Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 217, in _make_common_sync_call
    response = sync_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 979, in post
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 961, in post
    response.raise_for_status()
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 2126, in completion
    response = base_llm_http_handler.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 520, in completion
    response = self._make_common_sync_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 242, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 411, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 291, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1669, in call
    result = self._handle_non_streaming_response(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1150, in _handle_non_streaming_response
    response = litellm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1405, in wrapper
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1274, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 4080, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 429, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

03:35:02 | INFO     | __main__        | ‚úÖ Input capturado: 20 caracteres
03:35:02 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:35:02 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:35:02 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:35:05 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:35:05 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:35:07 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:35:07 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:37:37 | INFO     | __main__        | ‚úÖ Input capturado: 41 caracteres
03:37:37 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:37:37 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:37:38 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:37:40 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:37:40 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:37:42 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:37:42 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:37:43 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:37:43 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.1-8b-instant; provider = groq
03:37:43 | ERROR    | __main__        | Erro fatal: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5983, Requested 1808. Please try again in 17.91s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 217, in _make_common_sync_call
    response = sync_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 979, in post
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 961, in post
    response.raise_for_status()
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 2126, in completion
    response = base_llm_http_handler.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 520, in completion
    response = self._make_common_sync_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 242, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5983, Requested 1808. Please try again in 17.91s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 411, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 291, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1669, in call
    result = self._handle_non_streaming_response(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1150, in _handle_non_streaming_response
    response = litellm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1405, in wrapper
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1274, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 4080, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kdpptsa2faq97147v02v43r1` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5983, Requested 1808. Please try again in 17.91s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

03:38:50 | INFO     | __main__        | ‚úÖ Input capturado: 41 caracteres
03:38:50 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:38:50 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:40:48 | INFO     | __main__        | ‚úÖ Input capturado: 13 caracteres
03:40:48 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:40:48 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:40:48 | INFO     | LiteLLM         | 
LiteLLM completion() model= compound; provider = groq
03:41:40 | INFO     | __main__        | ‚úÖ Input capturado: 23 caracteres
03:41:45 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:41:45 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:41:46 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:41:46 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:41:46 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:41:46 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:41:46 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:41:46 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:41:46 | ERROR    | __main__        | Erro fatal: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 422, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 303, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 209, in call
    return self._handle_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 510, in _handle_completion
    raise e from e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 422, in _handle_completion
    response: ChatCompletion = self.client.chat.completions.create(**params)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:34 | INFO     | __main__        | ‚úÖ Input capturado: 23 caracteres
03:42:39 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:42:39 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:42:39 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:39 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:40 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:40 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:40 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:40 | ERROR    | root            | OpenAI API call failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:42:40 | ERROR    | __main__        | Erro fatal: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 423, in execute_task
    result = self.execute_task(task, context, tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 422, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 303, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 209, in call
    return self._handle_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 510, in _handle_completion
    raise e from e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llms\providers\openai\completion.py", line 422, in _handle_completion
    response: ChatCompletion = self.client.chat.completions.create(**params)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
03:44:50 | INFO     | __main__        | ‚úÖ Input capturado: 61 caracteres
03:44:50 | INFO     | __main__        | ‚úÖ Equipe de 6 especialistas criada
03:44:50 | INFO     | __main__        | ‚úÖ Pipeline de 6 etapas criado
03:44:51 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:44:54 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:44:54 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
03:44:57 | INFO     | LiteLLM         | Wrapper: Completed Call, calling success_handler
03:44:57 | INFO     | LiteLLM         | 
LiteLLM completion() model= llama-3.3-70b-versate; provider = groq
03:44:57 | ERROR    | __main__        | Erro fatal: litellm.NotFoundError: GroqException - {"error":{"message":"The model `llama-3.3-70b-versate` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}
Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 217, in _make_common_sync_call
    response = sync_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 979, in post
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 961, in post
    response.raise_for_status()
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 2126, in completion
    response = base_llm_http_handler.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 520, in completion
    response = self._make_common_sync_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 242, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `llama-3.3-70b-versate` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\bubbl\Desktop\Bubble_Agency_AI\main copy_claude..py", line 948, in main
    resultado = crew.kickoff()
                ^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 714, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1073, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\crew.py", line 1159, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 458, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 695, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\task.py", line 626, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 411, in execute_task
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 389, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agent\core.py", line 485, in _execute_without_timeout
    return self.agent_executor.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 192, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 291, in _invoke_loop
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 233, in _invoke_loop
    answer = get_llm_response(
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 276, in get_llm_response
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 268, in get_llm_response
    answer = llm.call(
             ^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1669, in call
    result = self._handle_non_streaming_response(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\crewai\llm.py", line 1150, in _handle_non_streaming_response
    response = litellm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1405, in wrapper
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\utils.py", line 1274, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\main.py", line 4080, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "C:\Users\bubbl\Desktop\Bubble_Agency_AI\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 387, in exception_type
    raise NotFoundError(
litellm.exceptions.NotFoundError: litellm.NotFoundError: GroqException - {"error":{"message":"The model `llama-3.3-70b-versate` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}

2025-12-30 03:46:23 | INFO     | ‚úÖ Configura√ß√£o validada
2025-12-30 03:46:30 | INFO     | ‚úÖ Input capturado: 61 caracteres
2025-12-30 03:46:30 | INFO     | Iniciando processamento de 61 caracteres
2025-12-30 03:46:30 | INFO     | ‚úÖ 4 agentes criados
2025-12-30 03:46:30 | INFO     | ‚úÖ Pipeline de 4 tarefas configurada
2025-12-30 03:46:30 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 03:46:35 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 03:46:35 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 03:46:36 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 03:46:36 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 03:46:37 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 03:46:37 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 03:46:38 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 03:46:38 | INFO     | üé® Gerando imagem: Abstract geometric shapes with gradient modern aesthetic ris...
2025-12-30 03:46:41 | INFO     | ‚úÖ Imagem salva: galeria_refinaria\post_2025_12_30__03_46_41.png
2025-12-30 03:46:41 | INFO     | 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-12-30 03:46:42 | INFO     | Wrapper: Completed Call, calling success_handler
2025-12-30 03:47:16 | INFO     | üìÅ Criando pasta: galeria_refinaria\post_2025_12_30__03_47_16
2025-12-30 03:47:16 | INFO     | ‚úÖ Post salvo: galeria_refinaria\post_2025_12_30__03_47_16\post.txt
2025-12-30 03:47:16 | INFO     | ‚úÖ Input original salvo: galeria_refinaria\post_2025_12_30__03_47_16\input_original.txt
2025-12-30 03:47:16 | INFO     | ‚úÖ Estrutura salva: galeria_refinaria\post_2025_12_30__03_47_16\estrutura.txt
2025-12-30 03:47:16 | INFO     | ‚úÖ Prompt de imagem salvo: galeria_refinaria\post_2025_12_30__03_47_16\prompt_imagem.txt
2025-12-30 03:47:16 | INFO     | ‚úÖ Imagem movida para: galeria_refinaria\post_2025_12_30__03_47_16\imagem.png
2025-12-30 03:47:16 | INFO     | ‚úÖ Metadata salva: galeria_refinaria\post_2025_12_30__03_47_16\metadata.json
2025-12-30 03:47:16 | INFO     | ‚úÖ README criado: galeria_refinaria\post_2025_12_30__03_47_16\README.md
2025-12-30 03:47:16 | INFO     | ‚úÖ Processamento conclu√≠do com sucesso
